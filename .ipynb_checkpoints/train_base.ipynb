{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path, scaler):\n",
    "        self.file_path = file_path\n",
    "        self.scaler = scaler()\n",
    "        self.data = pd.read_csv(file_path).values\n",
    "        self.data = self.scaler.transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd29503c10064c5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = # кол-во фич\n",
    "hidden_size = 64\n",
    "output_size = 5  # классификатор типы аномалий + норм\n",
    "\n",
    "\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(2):\n",
    "        folder_path = f\"path/to/your/csv/folder_{i}\"\n",
    "        file_list = os.listdir(folder_path)\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            dataset = CustomDataset(file_path, scaler)\n",
    "            dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "            for data in dataloader:\n",
    "                inputs = torch.tensor(data, dtype=torch.float32)\n",
    "                labels = # лейблы из датасета\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for i in range(2): \n",
    "        folder_path = f\"path/to/your/csv/folder_{i}\"\n",
    "        file_list = os.listdir(folder_path)\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            dataset = CustomDataset(file_path, scaler)\n",
    "            dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "            for data in dataloader:\n",
    "                inputs = torch.tensor(data, dtype=torch.float32)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.extend(predicted.numpy())\n",
    "               \n",
    "                all_labels.extend(dataset.labels)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6fc18ba3ca351e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f1b7e76ec4514c87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
